#!/usr/bin/env python3
"""Summarize read counts from stats TSV files.

Searches recursively in the provided directory for files generated by
``collect_read_stats.py``. These files have the suffixes
``*_raw_stats.tsv``, ``*_processed_stats.tsv`` and ``*_filtered_stats.tsv``.
For each unique prefix, count the number of reads (rows) in each file and
output a table with columns ``sample``, ``raw``, ``processed`` and
``filtered``. Optionally, sample names can be mapped to experiment names
through a metadata file with columns ``fastq`` and ``experiment``.
"""
from __future__ import annotations

import argparse
import csv
import glob
import os
import pathlib
import re
import sys
from collections import defaultdict


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument("directory", help="Directory containing stats TSV files")
    parser.add_argument(
        "--metadata", help="TSV/CSV with columns 'fastq' and 'experiment'", default=None
    )
    return parser.parse_args()


def load_metadata(path: str | None) -> dict[str, str]:
    mapping: dict[str, str] = {}
    if not path:
        return mapping
    with open(path) as fh:
        reader = csv.DictReader(fh, delimiter="\t")
        for row in reader:
            fastq = pathlib.Path(row["fastq"]).stem
            mapping[fastq] = row["experiment"]
    return mapping


args = parse_args()
base_dir = args.directory
metadata = load_metadata(args.metadata)

if not os.path.isdir(base_dir):
    print(f"Directory not found: {base_dir}", file=sys.stderr)
    sys.exit(1)

patterns = {
    "raw": "*_raw_stats.tsv",
    "processed": "*_processed_stats.tsv",
    "filtered": "*_filtered_stats.tsv",
}

counts = defaultdict(lambda: {"raw": 0, "processed": 0, "filtered": 0})

for stage, pattern in patterns.items():
    for path in glob.glob(os.path.join(base_dir, "**", pattern), recursive=True):
        file_name = os.path.basename(path)
        sample = re.sub(rf"_{stage}_stats\.tsv$", "", file_name)
        base = re.sub(r"^cleaned_", "", sample)
        base = re.sub(r"_trimmed$", "", base)  # unify trimmed filenames

        # Skip duplicate "cleaned_" files for stages other than "filtered"
        if sample.startswith("cleaned_") and stage != "filtered":
            continue

        try:
            with open(path) as fh:
                num = sum(1 for _ in fh) - 1  # subtract header
        except OSError as e:
            print(f"Warning: could not read {path}: {e}", file=sys.stderr)
            num = 0

        counts[base][stage] += num

print("sample\traw\tprocessed\tfiltered")
for sample in sorted(counts):
    name = metadata.get(sample, sample)
    data = counts[sample]
    print(f"{name}\t{data['raw']}\t{data['processed']}\t{data['filtered']}")
